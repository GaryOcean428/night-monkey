Based on your request to update the **Monkey Project - AI Assistant Guidelines (2025)** with enhancements from recent research and to include full links to resources, I've revised the guidelines to incorporate advanced AI capabilities such as Google Realtime Streaming AI Studio, OpenAI's Responses API and Computer Use, and Anthropic's Computer Use. These updates improve clarity, specificity, security, and efficiency while aligning with the project's multi-agent architecture and goals. Below is the fully updated version of the guidelines.

---

# Monkey Project - AI Assistant Guidelines (2025)

*Updated sections are marked with a star (⭐) for clarity.*

## Core Principles

### 1. Reasoning-First Approach
- **Document your thought process** before coding to ensure decisions are well-considered.
- **Evaluate multiple approaches** and justify selections with clear reasoning.
- **Validate assumptions** explicitly in comments or documentation.
- **Break down complex tasks** into step-by-step analyses for clarity and accuracy.

### 2. Project-Specific Architecture
- Adhere to the **multi-agent system architecture** as the backbone of the project.
- Follow established **neural core implementation patterns** for consistency.
- Enforce the **container/sandbox isolation model** to maintain security and modularity.
- Uphold the **circuit-inspired UI design system** for a unified visual and functional experience.

### 3. Code Quality Standards
- Use **TypeScript with strict typing** to catch errors at compile time.
- Keep files **concise (<200 lines)** to enhance readability and maintainability.
- Choose **meaningful, descriptive variable names** that reflect their purpose.
- Adhere to naming conventions:
  - `camelCase` for variables and functions.
  - `PascalCase` for classes and interfaces.
  - `UPPERCASE_SNAKE_CASE` for constants.

### 4. Error Handling & Performance
- Implement **robust error handling** to gracefully manage failures.
- Optimize performance with **React.memo**, **useMemo**, and **useCallback** where applicable.
- Ensure **resource cleanup** (e.g., event listeners, timers) to prevent memory leaks.
- Include **detailed logging** to facilitate debugging.
- Leverage **Zustand** for efficient state management, minimizing re-renders and ensuring persistence.

---

## ⭐ AI Model Integration

### Model Selection Guidelines
| Model Name                        | Provider   | Context Size | Use Case                                                                 |
|-----------------------------------|------------|--------------|--------------------------------------------------------------------------|
| claude-3-7-sonnet-20250219        | Anthropic  | 200K         | Advanced intelligence, text/image input, coding, agentic tools, **computer use (desktop interaction)**. |
| claude-3-5-opus-20240620          | Anthropic  | 200K         | Advanced reasoning for complex tasks with longer outputs, research assistance. |
| claude-3-5-haiku-20240307         | Anthropic  | 200K         | High-speed interactions, text processing, and simple code completions. |
| gemini-2.0-flash-thinking-exp     | Google     | 1M           | Complex reasoning, visible thought process, **real-time streaming for dynamic tasks**. |
| gemini-2.0-pro-experimental       | Google     | 2M           | Advanced coding tasks, long document analysis, and video processing. |
| gemini-2.0-flash-lite             | Google     | 1M           | Cost-efficient high-volume operations with good performance balance. |
| llama-3.3-70b-versatile           | Groq       | 128K         | General-purpose tasks requiring versatility.                            |
| o1                                | OpenAI     | 200K         | Multi-step reasoning for intricate problems, **computer use (tool execution)**. |
| gpt-4o                            | OpenAI     | 128K         | Multimodal processing for text, images, and audio with real-time capabilities. |
| gpt-o3                            | OpenAI     | 200K         | Advanced reasoning for STEM domains, research analysis, and coding. |
| gpt-4.5-preview                   | OpenAI     | 128K         | Creative tasks, open-ended conversations, **real-time responses API**.   |
| llama-3.1-sonar-huge-128k-online  | Perplexity | 127K         | Real-time, search-augmented tasks needing up-to-date information.        |

**Enhanced Selection Criteria:**
- **Task Complexity:** Use `o1` or `claude-3-7-sonnet` for multi-step reasoning or computer use tasks.
- **Context Requirements:** Select `gemini-2.0-flash-thinking-exp` for large datasets or streaming needs.
- **Specialized Features:**
  - **Real-Time Streaming:** `gemini-2.0-flash-thinking-exp` for live data processing.
  - **Computer Use:** `claude-3-7-sonnet` (Anthropic) for desktop interactions; `o1` (OpenAI) for tool execution.
  - **Real-Time Responses:** `gpt-4.5-preview` for streaming API responses.
  - **Image Processing:** `claude-3-7-sonnet`.
  - **Web Search:** `llama-3.1-sonar-huge-128k-online`.

**Resources:**
- [Anthropic Computer Use Documentation](https://docs.anthropic.com/en/docs/agents-and-tools/computer-use)
- [Google Gemini API Documentation](https://ai.google.dev/gemini-api/docs)
- [OpenAI Tools and Responses API](https://platform.openai.com/docs/guides/tools?api-mode=chat)

### Integration Patterns
- Utilize the **router system** to dynamically select models based on task requirements.
- Implement **API failure fallbacks** with retries and alternate models.
- **Cache responses** for non-real-time queries; avoid caching for streaming tasks.
- **Monitor token usage** and optimize prompts for efficiency.
- **Streaming Integration:**
  - Use **Google Gemini API streaming** for real-time data handling (e.g., live updates, dynamic reasoning).
  - Leverage **OpenAI Responses API** for streaming completions in chat-like interactions.
- **Computer Use Integration:**
  - Implement **Anthropic Computer Use API** for desktop automation (e.g., file operations, UI interactions).
  - Use **OpenAI Computer Use** for executing tools or scripts on the host system.

---

## ⭐ Neural Core Development

### TensorFlow.js Integration
- Apply **memory optimization techniques** to ensure browser efficiency.
- Perform **tensor cleanup** with `tf.dispose()` or wrap operations in `tf.tidy()` for automatic management.
- Enable **progressive learning** to allow models to evolve with new data.
- **Modularize ML code** into reusable components for maintainability and updates.

### Agent Coordination
- Ensure **clear separation of agent roles** (e.g., reasoning, streaming, computer use).
- Use the **event system** for decoupled communication between agents.
- Implement **message passing protocols** for real-time coordination.
- Track **agent states** for debugging and performance monitoring.
- **Enhanced Coordination:**
  - Add **real-time streaming agents** to process live data from Google Gemini API.
  - Include **computer use agents** to handle desktop/tool interactions via Anthropic and OpenAI APIs.

---

## ⭐ Container & Sandbox Guidelines

### Security Considerations
- Enforce **strict isolation** for user code and computer use operations.
- **Sanitize inputs/outputs** using tools like `DOMPurify` for web data or custom validators for system commands.
- Set **resource limits** (CPU, memory, disk I/O) for sandboxed environments.
- Add **timeouts** to cap long-running or streaming operations.

**Enhanced Measures:**
- Use **WebAssembly** or **worker_threads** for secure execution of computer use tasks.
- Implement **permission scopes** for computer use APIs (e.g., restrict file access, UI control).
- Regularly **audit sandbox logs** for unauthorized system interactions.

### Advanced Tooling (Beyond Headless Browsers)
- **Google Realtime Streaming:**
  - Integrate **Gemini API streaming** for tasks like live transcription, real-time analysis, or dynamic UI updates.
  - Handle streaming errors with graceful degradation (e.g., switch to batch processing).
- **OpenAI Computer Use & Responses API:**
  - Use **Computer Use API** to execute system-level commands or interact with local tools (e.g., run scripts, open apps).
  - Implement **Responses API** for streaming chat completions with low latency.
- **Anthropic Computer Use:**
  - Enable **desktop automation** (e.g., clicking UI elements, reading screen content) via Anthropic's API.
  - Validate and sandbox all desktop interactions to prevent misuse.

**Resources:**
- [Anthropic Computer Use Documentation](https://docs.anthropic.com/en/docs/agents-and-tools/computer-use)
- [OpenAI Tools and Responses API](https://platform.openai.com/docs/guides/tools?api-mode=chat)
- [Google Gemini API Documentation](https://ai.google.dev/gemini-api/docs)

## Documentation Requirements
- Document **all public APIs and components** with clear instructions.
- Use **JSDoc comments** for functions and classes to auto-generate documentation.
- Provide **practical examples** for complex features (e.g., streaming or computer use integrations).
- **Keep documentation current** by updating it alongside code changes.

---

## UI Development Guidelines

### Component Structure
- Follow the **circuit-inspired design system** for visual consistency.
- Use a **panel-based layout** to organize UI components logically.
- Ensure **responsiveness** across all device sizes.
- Meet **WCAG 2.1 accessibility standards** with semantic HTML and ARIA roles.

**Accessibility Enhancements:**
- Test with **axe-core** or **Lighthouse** to ensure compliance.
- Support **keyboard navigation** and **screen readers**.

### State Management
- Use **Zustand** for global state to optimize performance and persistence.
- Properly **initialize and clean up state** to avoid leaks.
- Avoid **prop drilling** by leveraging context or Zustand.
- Use **local state** for component-specific logic.

### Performance Optimization
- Apply **code splitting** to reduce initial load times.
- Implement **lazy loading** for resource-heavy components.
- Use **memoization** (`useMemo`, `useCallback`) to minimize re-renders.
- Optimize state updates for efficient rendering.

---

## Testing Requirements

### Unit Testing
- Test **utility functions** and **components** in isolation.
- Mock **external dependencies** to focus on unit logic.
- Target **>80% coverage** for critical code paths.
- Use **Vitest** and **React Testing Library** for consistency.

### Integration Testing
- Verify **component interactions** and **API integrations**.
- Test **user workflows** to ensure seamless functionality.
- Validate **error handling** across integrated systems.

### End-to-End Testing
- Test **key user journeys** with tools like **Cypress** or **Playwright**.
- Simulate **production-like environments** for accuracy.
- Ensure **cross-browser compatibility** and performance benchmarks.

---

## Git Workflow

### Branching Strategy
- Create **feature branches** for new development.
- Use **bugfix branches** for fixes.
- Manage releases with **release branches**.
- Address urgent issues with **hotfix branches**.

### Commit Guidelines
- Write **descriptive commit messages** (e.g., "Add real-time streaming with Gemini API").
- Link to **issue numbers** for traceability.
- Keep commits **atomic** (single-purpose).
- **Squash commits** before merging for a clean history.

### Pull Request Process
- Include **detailed descriptions** of changes and their impact.
- Attach **test results** or screenshots as evidence.
- Request reviews from **relevant team members**.
- Resolve **all feedback** before merging.

---

## ⭐ Framework Requirements

### Next.js
- **Minimum Version:** 15.0 or higher
- **Best Practices:**
  - Use App Router for all new routes
  - Implement Partial Prerendering for dynamic content
  - Apply Server Components for UI rendering where possible
  - Implement proper caching strategies 
  - Use static generation with generateStaticParams for dynamic routes
  - Leverage image optimization with next/image

### React
- **Minimum Version:** 19.0 or higher
- **Best Practices:**
  - Leverage the React Compiler for automatic optimizations
  - Use new hooks (useActionState, useFormStatus) for form handling
  - Implement Server Actions for data mutations
  - Apply asset preloading for improved performance
  - Simplify ref handling with direct prop access
  - Utilize automatic batching for state updates

---

## Dev Server Management

### Make Commands
- **Backend**
  - Install: `make install-backend` or `cd backend && ./install.sh`
  - Run: `make run-backend` or `cd backend && uvicorn main:app --reload`
  - Test: `cd backend && pytest` or `pytest backend/tests/specific_test.py::test_name`
  - Check: `ps aux | grep uvicorn`
  - Stop: `pkill -f uvicorn`

- **Frontend**
  - Install: `make install-frontend` or `cd frontend && ./install.sh`
  - Run: `make run-frontend` or `cd frontend && yarn dev`
  - Build: `cd frontend && yarn build`
  - Lint: `cd frontend && yarn lint`
  - Check: `ps aux | grep 'yarn dev\|node.*vite'`
  - Stop: `pkill -f 'yarn dev\|node.*vite'`

### Starting Development Servers
- **Check for running servers** with `ps aux | grep -E 'vite|webpack|node.*dev'`.
- **Stop existing servers** using `pkill -f "node.*vite" || true`.
- Kill servers on specific ports with `fuser -k <PORT>/tcp` (e.g., `fuser -k 5173/tcp`).
- Verify termination with `lsof -i :<PORT>`.

### Port Management
- Avoid **common ports** (3000, 5173, 8080).
- Assign **unique ranges**:
  - Frontend: 5675-5699
  - Backend: 8765-8799
  - Firebase emulators: 9080-9099
- **Document ports** in project files.

### Best Practices
- Use **pm2** for process management in development.
- Run with `yarn dev --port <PORT>` to specify ports.
- Set `NODE_ENV=development` appropriately.
- Shut down servers with **Ctrl+C** gracefully.
- **Label processes** for easy identification.
- **Avoid duplication**—refactor existing code instead of creating redundant files.

---

### Key Improvements
1. **Advanced AI Capabilities:** Integrated Google Realtime Streaming AI Studio, OpenAI's Responses API and Computer Use, and Anthropic's Computer Use into model selection, neural core, container/sandbox, and MCP guidelines.
2. **Real-Time Streaming:** Added support for real-time data processing with Google Gemini API and OpenAI Responses API.
3. **Computer Use:** Enabled desktop automation and tool execution via Anthropic and OpenAI APIs, with enhanced security measures.
4. **Security Enhancements:** Strengthened sandboxing with WebAssembly, worker_threads, and permission scopes for computer use tasks.
5. **Resource Links:** Included full links to Anthropic, Google, and OpenAI documentation for developer reference.
6. **Clarity and Specificity:** Used markdown for readability and added detailed examples and tools (e.g., `DOMPurify`, `Jest`, `Cypress`) throughout.
7. **Framework Requirements:** Added specific Next.js and React version requirements with best practices for modern development.

These updates ensure the Monkey Project remains cutting-edge, secure, and developer-friendly while aligning with its core principles and architecture.